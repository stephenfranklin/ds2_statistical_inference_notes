<!DOCTYPE html>
<!-- saved from url=(0014)about:internet -->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=9" >

<title>conditional_probability.Rnw</title>

<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 12px;
   margin: 8px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 { 
   font-size:2.2em; 
}

h2 { 
   font-size:1.8em; 
}

h3 { 
   font-size:1.4em; 
}

h4 { 
   font-size:1.0em; 
}

h5 { 
   font-size:0.9em; 
}

h6 { 
   font-size:0.8em; 
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre {	
   margin-top: 0;
   max-width: 95%;
   border: 1px solid #ccc;
   white-space: pre-wrap;
}

pre code {
   display: block; padding: 0.5em;
}

code.r, code.cpp {
   background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * { 
      background: transparent !important; 
      color: black !important; 
      filter:none !important; 
      -ms-filter: none !important; 
   }

   body { 
      font-size:12pt; 
      max-width:100%; 
   }
       
   a, a:visited { 
      text-decoration: underline; 
   }

   hr { 
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote { 
      padding-right: 1em; 
      page-break-inside: avoid; 
   }

   tr, img { 
      page-break-inside: avoid; 
   }

   img { 
      max-width: 100% !important; 
   }

   @page :left { 
      margin: 15mm 20mm 15mm 10mm; 
   }
     
   @page :right { 
      margin: 15mm 10mm 15mm 20mm; 
   }

   p, h2, h3 { 
      orphans: 3; widows: 3; 
   }

   h2, h3 { 
      page-break-after: avoid; 
   }
}

</style>



<!-- MathJax scripts -->
<script type="text/javascript" src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



</head>

<body>
<h1>conditional_probability.Rnw</h1>

<p>R markdown / LaTeX notes by Stephen Franklin, May 2014,
making extensive use of the github notes
<a href="https://github.com/DataScienceSpecialization/courses/blob/master/06_StatisticalInference/01_05_ConditionalProbability/index.Rmd">https://github.com/DataScienceSpecialization/courses/blob/master/06_StatisticalInference/01_05_ConditionalProbability/index.Rmd</a>
for Statistical Inference - Coursera / Johns Hopkins - Prof. Brian Caffo</p>

<h2>Conditional probability, motivation</h2>

<ul>
<li>The probability of getting a one when rolling a (standard) die
is usually assumed to be one sixth</li>
<li>Suppose you were given the extra information that the die roll
was an odd number (hence 1, 3 or 5)</li>
<li><em>conditional on this new information</em>, the probability of a
one is now one third</li>
</ul>

<hr/>

<h2>Conditional probability, definition</h2>

<ul>
<li>Let \( B \) be an event so that \( P(B) > 0 \)

<ul>
<li>You can&#39;t condition on a 0 probability event, i.e. on the die rolling a 7.</li>
</ul></li>
<li><p>Then the conditional probability of an event \( A \) given that \( B \) has occurred is
\[ 
  P(A ~|~ B) = \frac{P(A \cap B)}{P(B)}
   \]</p>

<ul>
<li>The pipe symbol (\( ~|~ \)) is read as &ldquo;given,&rdquo; i.e. &ldquo;The probability of A given B&hellip;&rdquo;</li>
</ul></li>
<li><p>Notice that if \( A \) and \( B \) are independent, then
\[ 
  P(A ~|~ B) = \frac{P(A) P(B)}{P(B)} = P(A)
   \]</p>

<ul>
<li>And that is actually an alternative definition of independence.</li>
</ul></li>
</ul>

<hr/>

<h2>Example</h2>

<ul>
<li>Consider our die roll example</li>
<li>\( B = \{1, 3, 5\} \)</li>
<li>\( A = \{1\} \)
\[ 
  \begin{eqnarray*}
P(\mbox{one given that roll is odd})  & = & P(A ~|~ B) \\ \\
  & = & \frac{P(A \cap B)}{P(B)} \\ \\
  & = & \frac{P(A)}{P(B)} \\ \\ 
  & = & \frac{1/6}{3/6} = \frac{1}{3}
  \end{eqnarray*}
 \]</li>
<li>Note that A is a subset of B, \( A \subset B \), so the probability of A intersect B, \( P(A \cap B) \), is just \( P(A) \).</li>
</ul>

<hr/>

<h2>Bayes&#39; rule</h2>

<ul>
<li>Bayes&#39; rule allows you to flip what your conditioning on.
\[ 
P(B ~|~ A) = \frac{P(A ~|~ B) P(B)}{P(A ~|~ B) P(B) + P(A ~|~ B^c)P(B^c)}.
 \]</li>
<li>It&#39;s a clever expansion of the conditional probability equation for \( P(B ~|~ A) \).</li>
<li>The numerator comes from the conditional probability equation (above) of \( P(A \cap B) = P(B \cap A) \).

<ul>
<li>The idea is that \( P(A ~|~ B) \) is given, and \( P(B) \) is given, and you want \( P(B ~|~ A) \).</li>
</ul></li>
<li>The denominator comes from a Kolmogorov axiom: \( P(A \cap \bar{B}) = P(A) - P(A \cap B) \), the two addends are then run through the conditional probability equation again.

<ul>
<li>Explore that equation by drawing a venn diagram of it, or watch this lecture. </li>
</ul></li>
</ul>

<hr/>

<h2>Diagnostic tests</h2>

<ul>
<li>Let \( + \) and \( - \) be the events that the result of a diagnostic test is positive or negative respectively</li>
<li>Let \( D \) and \( D^c \) be the event that the subject of the test has or does not have the disease respectively </li>
<li>The <strong>sensitivity</strong> is the probability that the test is positive given that the subject actually has the disease, \( P(+ ~|~ D) \)</li>
<li>The <strong>specificity</strong> is the probability that the test is negative given that the subject does not have the disease, \( P(- ~|~ D^c) \)</li>
</ul>

<hr/>

<h2>More definitions</h2>

<ul>
<li>The <strong>positive predictive value</strong> is the probability that the subject has the disease given that the test is positive,

<ul>
<li>\( P(D ~|~ +) \)</li>
<li>\( 1-P(D ~|~ +) \) is the probability of not having the disease given a positive test result.</li>
</ul></li>
<li>The <strong>negative predictive value</strong> is the probability that the subject does not have the disease given that the test is negative,

<ul>
<li>\( P(D^c ~|~ -) \)</li>
</ul></li>
<li>The <strong>prevalence of the disease</strong> is the marginal probability of disease,

<ul>
<li>\( P(D) \)</li>
</ul></li>
</ul>

<hr/>

<h2>More definitions</h2>

<ul>
<li>The <strong>diagnostic likelihood ratio of a positive test</strong>, labeled \( DLR_+ \), is \( P(+ ~|~ D) / P(+ ~|~ D^c) \), which is the \[ \sf{sensitivity} / (1 - specificity) \]</li>
<li>The <strong>diagnostic likelihood ratio of a negative test</strong>, labeled \( DLR_- \), is \( P(- ~|~ D) / P(- ~|~ D^c) \), which is the \[ (1 - \sf{sensitivity}) / specificity \]</li>
</ul>

<hr/>

<h2>Example</h2>

<ul>
<li>A study comparing the efficacy of HIV tests, reports on an experiment which concluded that HIV antibody tests have a sensitivity of 99.7% and a specificity of 98.5%</li>
<li>Suppose that a subject, from a population with a .1% prevalence of HIV, receives a positive test result. What is the probability that this subject has HIV?</li>
<li>Mathematically, we want \( P(D ~|~ +) \) given the sensitivity, \( P(+ ~|~ D) = .997 \), the specificity, \( P(- ~|~ D^c) =.985 \), and the prevalence \( P(D) = .001 \)</li>
</ul>

<hr/>

<h2>Using Bayes&#39; formula</h2>

<p>\[ 
\begin{eqnarray*}
  P(D ~|~ +) & = &\frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)}\\ \\
 & = & \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + \{1-P(-~|~D^c)\}\{1 - P(D)\}} \\ \\
 & = & \frac{.997\times .001}{.997 \times .001 + .015 \times .999}\\ \\
 & = & .062
\end{eqnarray*}
 \]</p>

<ul>
<li>In this population a positive test result only suggests a 6% probability that the subject has the disease </li>
<li>(The positive predictive value is 6% for this test)</li>
</ul>

<hr/>

<h2>More on this example</h2>

<ul>
<li>The low positive predictive value is due to low prevalence of disease and the somewhat modest specificity</li>
<li>Suppose it was known that the subject was an intravenous drug user and routinely had intercourse with an HIV infected partner</li>
<li>Notice that the evidence implied by a positive test result does not change because of the prevalence of disease in the subject&#39;s population, only our interpretation of that evidence changes</li>
</ul>

<hr/>

<h2>Likelihood ratios</h2>

<ul>
<li>Using Bayes rule, we have
\[ 
  P(D ~|~ +) = \frac{P(+~|~D)P(D)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)} 
   \]
and
\[ 
  P(D^c ~|~ +) = \frac{P(+~|~D^c)P(D^c)}{P(+~|~D)P(D) + P(+~|~D^c)P(D^c)}.
   \]</li>
</ul>

<hr/>

<h2>Likelihood ratios</h2>

<ul>
<li>Therefore
\[ 
\frac{P(D ~|~ +)}{P(D^c ~|~ +)} = \frac{P(+~|~D)}{P(+~|~D^c)}\times \frac{P(D)}{P(D^c)}
 \]
ie
\[ 
\mbox{post-test odds of }D = DLR_+\times\mbox{pre-test odds of }D
 \]</li>
<li>Similarly, \( DLR_- \) relates the decrease in the odds of the
disease after a negative test result to the odds of disease prior to
the test.</li>
</ul>

<hr/>

<h2>HIV example revisited</h2>

<ul>
<li>Suppose a subject has a positive HIV test</li>
<li>\( DLR_+ = .997 / (1 - .985) \approx 66 \)</li>
<li>The result of the positive test is that the odds of disease is now 66 times the pretest odds</li>
<li>Or, equivalently, the hypothesis of disease is 66 times more supported by the data than the hypothesis of no disease</li>
</ul>

<hr/>

<h2>HIV example revisited</h2>

<ul>
<li>Suppose that a subject has a negative test result </li>
<li>\( DLR_- = (1 - .997) / .985  \approx .003 \)</li>
<li>Therefore, the post-test odds of disease is now \( .3\% \) of the pretest odds given the negative test.</li>
<li>Or, the hypothesis of disease is supported \( .003 \) times that of the hypothesis of absence of disease given the negative test result
\end{document}</li>
</ul>

</body>

</html>

